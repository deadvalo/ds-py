{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf89ba1",
   "metadata": {},
   "source": [
    "### This notebook aims at providing a pipeline for a binary CNN classifier but can be used for any model of the keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9c556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageOps\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd366d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please change the directories to the one where you downloaded the datasets\n",
    "train_dir = \"C:/Users/thoma/OneDrive/Desktop/Centrale/3A/Spé info/SID/Méthodo/Projet/training\"\n",
    "val_dir = \"C:/Users/thoma/OneDrive/Desktop/Centrale/3A/Spé info/SID/Méthodo/Projet/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e411101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We resize the images to a 256x256 format\n",
    "def resize_image(img):\n",
    "    res = img.resize((256,256),Image.BICUBIC)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3dfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, train_dir, val_dir):\n",
    "        self.train_dir = train_dir\n",
    "        self.val_dir = val_dir\n",
    "        \n",
    "    def load_data(self):\n",
    "        \n",
    "        Training_img = []\n",
    "        \n",
    "        Dmg_train_dir = self.train_dir + \"/00-damage\"\n",
    "        \n",
    "        for images in os.listdir(Dmg_train_dir):\n",
    "            im = Image.open(Dmg_train_dir + '/' + images)\n",
    "            Training_img.append([im,1])\n",
    "            \n",
    "        Whole_train_dir = self.train_dir + \"/01-whole\"\n",
    "        \n",
    "        for images in os.listdir(Whole_train_dir):\n",
    "            im = Image.open(Whole_train_dir + '/' + images)\n",
    "            Training_img.append([im,0])\n",
    "            \n",
    "        random.shuffle(Training_img)\n",
    "        \n",
    "        #We needed to take images and labels as a whole as to shuffle them so as to avoid any bias when training models\n",
    "        \n",
    "        X_train,y_train = [],[]\n",
    "        \n",
    "        for k in Training_img:\n",
    "            X_train.append(k[0])\n",
    "            y_train.append(k[1])\n",
    "        \n",
    "        X_train_keras = np.array([np.asarray(resize_image(img)) for img in X_train])\n",
    "        y_train_keras = np.array(y_train)\n",
    "        \n",
    "        Validation_img = []\n",
    "        \n",
    "        Dmg_val_dir = self.val_dir + \"/00-damage\"\n",
    "        \n",
    "        for images in os.listdir(Dmg_val_dir):\n",
    "            im = Image.open(Dmg_val_dir + '/' + images)\n",
    "            Validation_img.append([im,1])\n",
    "            \n",
    "        Whole_val_dir = self.val_dir + \"/01-whole\"\n",
    "        \n",
    "        for images in os.listdir(Whole_val_dir):\n",
    "            im = Image.open(Whole_val_dir + '/' + images)\n",
    "            Validation_img.append([im,0])\n",
    "            \n",
    "        random.shuffle(Validation_img)\n",
    "        \n",
    "        #Same reason as above for this part of the code\n",
    "        \n",
    "        X_val,y_val = [],[]\n",
    "        \n",
    "        for k in Validation_img:\n",
    "            X_val.append(k[0])\n",
    "            y_val.append(k[1])\n",
    "            \n",
    "        X_val_keras = np.array([np.asarray(resize_image(img)) for img in X_val])\n",
    "        y_val_keras = np.array(y_val)\n",
    "        \n",
    "        return X_train_keras, y_train_keras, X_val_keras, y_val_keras                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496dab81",
   "metadata": {},
   "source": [
    "We chose a LeNet-5 architecture of CNN for two reasons:\n",
    "\n",
    " First, we think it is a nice reference to the work of a great french researcher, Mr Yann LeCun. \n",
    " Second, since it was one the first CNN model it is surely not the most efficient to date and thus we can have comparison with more classical ML models (less designated for object detection than CNNs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aac276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(6, (5, 5), padding='same', input_shape=(256, 256, 3)),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    keras.layers.Conv2D(16, (5, 5)),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(120),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(84),\n",
    "    keras.layers.Activation('relu'),\n",
    "    keras.layers.Dense(1),\n",
    "    keras.layers.Activation('sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f349e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with a binary cross entropy since we only have two categories (damaged and whole)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e8aac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "58/58 [==============================] - 27s 446ms/step - loss: 32.7628 - accuracy: 0.6188\n",
      "Epoch 2/5\n",
      "58/58 [==============================] - 26s 455ms/step - loss: 0.3453 - accuracy: 0.8657\n",
      "Epoch 3/5\n",
      "58/58 [==============================] - 25s 437ms/step - loss: 0.1272 - accuracy: 0.9636\n",
      "Epoch 4/5\n",
      "58/58 [==============================] - 24s 422ms/step - loss: 0.0404 - accuracy: 0.9918\n",
      "Epoch 5/5\n",
      "58/58 [==============================] - 25s 428ms/step - loss: 0.0261 - accuracy: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e12cf42ca0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model\n",
    "X_train, y_train, X_val, y_val = DataLoader(train_dir, val_dir).load_data()\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e5442e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 74ms/step - loss: 1.3246 - accuracy: 0.7174\n",
      "test loss, test acc: [1.3245657682418823, 0.717391312122345]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_val, y_val, batch_size=32)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
